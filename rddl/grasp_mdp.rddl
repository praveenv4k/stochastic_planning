domain grasp_mdp {
	requirements = {
		reward-deterministic
	};
	
	types {
		// Robot Position and Orientation
		r_xpos : object;
		r_ypos : object;
		r_zpos : object;
		r_roll : object;
		r_pitch: object;
		r_yaw  : object;
		// Object Position
		o_xpos : object;
      		o_ypos : object;
   		o_zpos : object;
	};
	
	pvariables {
		MIN-XPOS(xpos) : {non-fluent, bool, default = false};
		MAX-XPOS(xpos) : {non-fluent, bool, default = false};
		MIN-YPOS(ypos) : {non-fluent, bool, default = false};
		MAX-YPOS(ypos) : {non-fluent, bool, default = false};
		MIN_ZPOS(zpos) : {non-fluent, bool, default = false};
		MAX_ZPOS(zpos) : {non-fluent, bool, default = false};

		REWARD(r_xpos, r_ypos, r_zpos, r_roll, r_pitch, r_yaw, o_xpos, o_ypos, o_zpos) : {non-fluent, real, default = -10};

		// Fluents
		state-at(r_xpos, r_ypos, r_zpos, r_roll, r_pitch, r_yaw, o_xpos, o_ypos, o_zpos) : {state-fluent, bool, default = false};
		
		// Actions
		action1 : {action-fluent, bool, default = false};
		action2 : {action-fluent, bool, default = false};
		action3  : {action-fluent, bool, default = false};
		action4  : {action-fluent, bool, default = false};
		actionN : {action-fluent, bool, default = false};
	};
	
	cpfs {
	
		state-at'(?r_x,?r_y,?r_z,?r_roll,?r_pitch,?r_yaw,?o_x,?o_y,?o_z) =
		
			if ( state-at(0.11,0.2,...) ^ action1 )
			then 
				Bernoulli(0.1);
	};
	
	reward = REWARD(r_xpos, r_ypos, r_zpos,r_roll, r_pitch, r_yaw, o_xpos,o_ypos,o_zpos);

}
